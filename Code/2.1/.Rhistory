store.beta2 <- tibble(beta2 = numeric())
store.sigma2  <- add_row(store.sigma2, sigma2 = current.sigma2)
store.beta2<- add_row(store.beta2, beta2 = current.beta2)
for(k in 2:num.iteration){
current.state <- c(rnorm(1), current.state)
current.sigma2 <- rinvgamma(n = 1, shape = 0.01 + T/2,
rate = 0.01 + 0.5 * sum((current.state[2:(T+1)] - current.state[1:T])^2))
current.beta2 <- rinvgamma(n = 1, shape = 0.01 + T/2,
rate = 0.01 + 0.5 * sum(exp(-current.state[2:(T+1)])*((y.values[1:T])^2)))
store.sigma2 <- add_row(store.sigma2, sigma2 = sqrt(current.sigma2))
store.beta2 <- add_row(store.beta2, beta2 = sqrt(current.beta2))
# use these new parametrs in CSMC
current.state <- as.numeric(csmc(current.state, theta=c(sqrt(current.sigma2), sqrt(current.beta2)), y.values, 100, 100)$x.means) # careful. convert it into vector
}
list(a = store.sigma2, b = store.beta2)
}
results.gibbs <- particle.gibbs(real_data$y, c(0.5, 0.5), 100, 100,1000)
ggplot(results.gibbs$a[100:1000, ], aes(sigma2)) +
geom_histogram()
ggplot(results.gibbs$b[100:1000, ], aes(beta2)) +
geom_histogram()
ggplot(results.gibbs$a[100:1000, ], aes(sigma2)) +
geom_histogram()
results.gibbs <- particle.gibbs(real_data$y, c(0.5, 0.5), 100, 100, 2000)
ggplot(results.gibbs$a[100:1000, ], aes(sigma2)) +
geom_histogram()
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
particle.gibbs <- function(y.values, theta, T, num.particles, num.iteration){
current.sigma2 <- theta[1]
current.beta2 <- theta[2]
# set x values arbitrarily
current.state <- generate_data(1, sqrt(current.sigma2),sqrt(current.beta2),100) %>% pull(x)
store.sigma2 <- tibble(sigma2 = numeric())
store.beta2 <- tibble(beta2 = numeric())
store.sigma2  <- add_row(store.sigma2, sigma2 = current.sigma2)
store.beta2<- add_row(store.beta2, beta2 = current.beta2)
for(k in 2:num.iteration){
current.state <- c(0, current.state)
current.sigma2 <- rinvgamma(n = 1, shape = 0.01 + T/2,
rate = 0.01 + 0.5 * sum((current.state[2:(T+1)] - current.state[1:T])^2))
current.beta2 <- rinvgamma(n = 1, shape = 0.01 + T/2,
rate = 0.01 + 0.5 * sum(exp(-current.state[2:(T+1)])*((y.values[1:T])^2)))
store.sigma2 <- add_row(store.sigma2, sigma2 = sqrt(current.sigma2))
store.beta2 <- add_row(store.beta2, beta2 = sqrt(current.beta2))
# use these new parametrs in CSMC
current.state <- as.numeric(csmc(current.state, theta=c(sqrt(current.sigma2), sqrt(current.beta2)), y.values, 100, 100)$x.means) # careful. convert it into vector
}
list(a = store.sigma2, b = store.beta2)
}
results.gibbs <- particle.gibbs(real_data$y, c(0.5, 0.5), 100, 100, 2000)
ggplot(results.gibbs$a[100:1000, ], aes(sigma2)) +
geom_histogram()
ggplot(results.gibbs$b[100:1000, ], aes(beta2)) +
geom_histogram()
library(tidyverse)
library(MCMCpack)
data <- read.table("input.csv", sep = " ", stringsAsFactors = T, header = F, nrows = 25)
K <- 4 # alphabet
W <- 10 # length of a magic word
N <- 25 # number of rows in data == number of sequences
M <- 1000 # length of one sequence
B <- N * (M - W)
alphabet <- c("a", "c", "g", "t")
alpha.bg <- read.table("alphaBg.csv", stringsAsFactors = T, header = F, col.names = "alpha")
alpha.bg <- alpha.bg$alpha # turn into vector
alpha.mw <- read.table("alphaMw.csv", stringsAsFactors = T, header = F, col.names = "alpha")
alpha.mw <- alpha.mw$alpha
View(data)
data <- read.table("input.csv", sep = " ", stringsAsFactors = T, header = F, nrows = 25)
K <- 4 # alphabet
W <- 10 # length of a magic word
N <- 25 # number of rows in data == number of sequences
M <- 1000 # length of one sequence
B <- N * (M - W)
alphabet <- c("a", "c", "g", "t")
alpha.bg <- read.table("alphaBg.csv", stringsAsFactors = T, header = F, col.names = "alpha")
alpha.bg <- alpha.bg$alpha # turn into vector
alpha.mw <- read.table("alphaMw.csv", stringsAsFactors = T, header = F, col.names = "alpha")
alpha.mw <- alpha.mw$alpha
posterior.tilda <- function(seq.id, gibbs.state){
positions <- gibbs.state
end.positions <- positions + W - 1
data.seq <- as.matrix(data, nrow = N, ncol = M)
mw.data.seq <- matrix(nrow = N, ncol = W)
for(row in 1:nrow(data.seq)){
mw.data.seq[row, ] <- data.seq[row, positions[row]:end.positions[row]]
}
# Background
bg.data.seq <- matrix(nrow = N, ncol = M - W)
for(row in 1:nrow(data.seq)){
bg.data.seq[row, ] <- data.seq[row, -c(positions[row]:end.positions[row])]
}
prob.vector <- c()
for(r.i in 1:(M - W + 1)){ # problematic
# my problems could be solved by avoiding dividing the data in each iteration
#positions <- gibbs.state # for ALL seq; this is collapsed Gibbs so it makes sense, r_n | R_{-n}
positions[seq.id] <- r.i # this is modifying only r_n
end.positions.for.rn <- positions[seq.id] + W - 1
# position is changed only for seq.id row, r_n
mw.data.seq[seq.id] <- data.seq[seq.id, ]
bg.data.seq[seq.id, ] <- data.seq[seq.id, -c(positions[seq.id]:end.positions.for.rn)]
prob.background <- bg.prob(bg.data.seq)
prob.mw <- 0
i <- 1
while(i != W){ # not a problem since W = 10
prob.mw <- prob.mw +  jth.mw.prob(mw.data.seq, i)
i <- i+1
}
prob.total <- prob.background + prob.mw
prob.vector <- c(prob.vector, prob.total)
}
prob.vector
}
# for j-th position
jth.mw.prob <- function(data, j){
counts <- table(factor(data[, j], levels = c("a", "c", "g", "t"))) %>% as.numeric()
part1 <- lgamma(sum(alpha.mw)) - lgamma(N * sum(alpha.mw))
part2 <- sum(lgamma(counts + alpha.mw) - lgamma(alpha.mw))
log.p <- part1 + part2
return(log.p)
}
# for j-th position
bg.prob  <- function(data){
counts <- table(factor(data, levels = c("a", "c", "g", "t")))%>% as.numeric()
part1 <- lgamma(sum(alpha.bg)) - lgamma(B * sum(alpha.bg))
part2 <- sum(lgamma(counts + alpha.bg) - lgamma(alpha.bg))
log.p <- part1 + part2
return(log.p)
}
# Magic word
data.seq <- as.matrix(data, nrow = N, ncol = M)
mw.positions <- initial.positions
# for j-th position
jth.mw.prob <- function(data, j){
counts <- table(factor(data[, j], levels = c("a", "c", "g", "t"))) %>% as.numeric()
part1 <- lgamma(sum(alpha.mw)) - lgamma(N * sum(alpha.mw))
part2 <- sum(lgamma(counts + alpha.mw) - lgamma(alpha.mw))
log.p <- part1 + part2
return(log.p)
}
# for j-th position
jth.mw.prob <- function(data, j){
counts <- table(factor(data[, j], levels = c("a", "c", "g", "t"))) %>% as.numeric()
part1 <- lgamma(sum(alpha.mw)) - lgamma(N * sum(alpha.mw))
part2 <- sum(lgamma(counts + alpha.mw) - lgamma(alpha.mw))
log.p <- part1 + part2
return(log.p)
}
# for j-th position
bg.prob  <- function(data){
counts <- table(factor(data, levels = c("a", "c", "g", "t")))%>% as.numeric()
part1 <- lgamma(sum(alpha.bg)) - lgamma(B * sum(alpha.bg))
part2 <- sum(lgamma(counts + alpha.bg) - lgamma(alpha.bg))
log.p <- part1 + part2
return(log.p)
}
posterior.tilda <- function(seq.id, gibbs.state){
positions <- gibbs.state
end.positions <- positions + W - 1
data.seq <- as.matrix(data, nrow = N, ncol = M)
mw.data.seq <- matrix(nrow = N, ncol = W)
for(row in 1:nrow(data.seq)){
mw.data.seq[row, ] <- data.seq[row, positions[row]:end.positions[row]]
}
# Background
bg.data.seq <- matrix(nrow = N, ncol = M - W)
for(row in 1:nrow(data.seq)){
bg.data.seq[row, ] <- data.seq[row, -c(positions[row]:end.positions[row])]
}
prob.vector <- c()
for(r.i in 1:(M - W + 1)){ # problematic
# my problems could be solved by avoiding dividing the data in each iteration
#positions <- gibbs.state # for ALL seq; this is collapsed Gibbs so it makes sense, r_n | R_{-n}
positions[seq.id] <- r.i # this is modifying only r_n
end.positions.for.rn <- positions[seq.id] + W - 1
# position is changed only for seq.id row, r_n
mw.data.seq[seq.id] <- data.seq[seq.id, ]
bg.data.seq[seq.id, ] <- data.seq[seq.id, -c(positions[seq.id]:end.positions.for.rn)]
prob.background <- bg.prob(bg.data.seq)
prob.mw <- 0
i <- 1
while(i != W){ # not a problem since W = 10
prob.mw <- prob.mw +  jth.mw.prob(mw.data.seq, i)
i <- i+1
}
prob.total <- prob.background + prob.mw
prob.vector <- c(prob.vector, prob.total)
}
prob.vector
}
exp.normalize <- function(prob){
prob <- exp(prob - max(prob))
return(prob/sum(prob))
}
gibbs.sampler <- function(num.iteration){
require(tidyverse)
initial.positions <- sample(1:(M - W + 1), N, replace = T) # for EVERY sequence
samples <- matrix(nrow = 1, ncol = N)
samples[1, ] <- initial.positions
for(i in 1:num.iteration){
position <- matrix(nrow = 1, ncol = N) # new "batch" for position for EVERY iteration
gibbs.state <- samples[i, ] # the latest
for(n in 1:N){ # for every sequence
posterior <- posterior.tilda(i, gibbs.state) # extremely slow !!!!!!!!!!!!!!!!!
posterior <- exp.normalize(posterior)
# multisample
print(length(posterior))
max.index <- sample(1:(M - W + 1), size = 1, prob=posterior)
gibbs.state[n] <- max.index # careful
position[1, i] <- max.index
}
samples <- rbind(samples, position) # save latest "batch" of positions
}
# chain
return(samples)
}
profvis({results <- gibbs.sampler(num.iteration = 1)})
results <- gibbs.sampler(num.iteration = 1)
debugSource('D:/Materijali/KTH/StatMethods/Projekt/Code/2.1/forDebugging.R')
warnings()
warninigs()
warnings()
warnings()
posterior.tilda <- function(seq.id, gibbs.state){
positions <- gibbs.state
end.positions <- positions + W - 1
data.seq <- as.matrix(data, nrow = N, ncol = M)
mw.data.seq <- matrix(nrow = N, ncol = W)
for(row in 1:nrow(data.seq)){
mw.data.seq[row, ] <- data.seq[row, positions[row]:end.positions[row]]
}
# Background
bg.data.seq <- matrix(nrow = N, ncol = M - W)
for(row in 1:nrow(data.seq)){
bg.data.seq[row, ] <- data.seq[row, -c(positions[row]:end.positions[row])]
}
prob.vector <- c()
for(r.i in 1:(M - W + 1)){ # problematic
# my problems could be solved by avoiding dividing the data in each iteration
#positions <- gibbs.state # for ALL seq; this is collapsed Gibbs so it makes sense, r_n | R_{-n}
positions[seq.id] <- r.i # this is modifying only r_n
end.positions.for.rn <- positions[seq.id] + W - 1
# position is changed only for seq.id row, r_n
mw.data.seq[seq.id] <- data.seq[seq.id, positions[seq.id]:end.positions.for.rn]
bg.data.seq[seq.id, ] <- data.seq[seq.id, -c(positions[seq.id]:end.positions.for.rn)]
prob.background <- bg.prob(bg.data.seq)
prob.mw <- 0
i <- 1
while(i != W){ # not a problem since W = 10
prob.mw <- prob.mw +  jth.mw.prob(mw.data.seq, i)
i <- i+1
}
prob.total <- prob.background + prob.mw
prob.vector <- c(prob.vector, prob.total)
}
prob.vector
}
exp.normalize <- function(prob){
prob <- exp(prob - max(prob))
return(prob/sum(prob))
}
results <- gibbs.sampler(num.iteration = 1)
debugSource('D:/Materijali/KTH/StatMethods/Projekt/Code/2.1/forDebugging.R')
warnings()
positions[seq.id]
warninigs()
mw.data.seq[seq.id]
posterior.tilda <- function(seq.id, gibbs.state){
positions <- gibbs.state
end.positions <- positions + W - 1
data.seq <- as.matrix(data, nrow = N, ncol = M)
mw.data.seq <- matrix(nrow = N, ncol = W)
for(row in 1:nrow(data.seq)){
mw.data.seq[row, ] <- data.seq[row, positions[row]:end.positions[row]]
}
# Background
bg.data.seq <- matrix(nrow = N, ncol = M - W)
for(row in 1:nrow(data.seq)){
bg.data.seq[row, ] <- data.seq[row, -c(positions[row]:end.positions[row])]
}
prob.vector <- c()
for(r.i in 1:(M - W + 1)){ # problematic
# my problems could be solved by avoiding dividing the data in each iteration
#positions <- gibbs.state # for ALL seq; this is collapsed Gibbs so it makes sense, r_n | R_{-n}
positions[seq.id] <- r.i # this is modifying only r_n
end.positions.for.rn <- positions[seq.id] + W - 1
# position is changed only for seq.id row, r_n
mw.data.seq[seq.id] <- data.seq[seq.id, positions[seq.id]:end.positions.for.rn]
bg.data.seq[seq.id, ] <- data.seq[seq.id, -c(positions[seq.id]:end.positions.for.rn)]
prob.background <- bg.prob(bg.data.seq)
prob.mw <- 0
i <- 1
while(i != W){ # not a problem since W = 10
prob.mw <- prob.mw +  jth.mw.prob(mw.data.seq, i)
i <- i+1
}
prob.total <- prob.background + prob.mw
prob.vector <- c(prob.vector, prob.total)
}
prob.vector
}
posterior.tilda <- function(seq.id, gibbs.state){
positions <- gibbs.state
end.positions <- positions + W - 1
data.seq <- as.matrix(data, nrow = N, ncol = M)
mw.data.seq <- matrix(nrow = N, ncol = W)
for(row in 1:nrow(data.seq)){
mw.data.seq[row, ] <- data.seq[row, positions[row]:end.positions[row]]
}
# Background
bg.data.seq <- matrix(nrow = N, ncol = M - W)
for(row in 1:nrow(data.seq)){
bg.data.seq[row, ] <- data.seq[row, -c(positions[row]:end.positions[row])]
}
prob.vector <- c()
for(r.i in 1:(M - W + 1)){ # problematic
# my problems could be solved by avoiding dividing the data in each iteration
#positions <- gibbs.state # for ALL seq; this is collapsed Gibbs so it makes sense, r_n | R_{-n}
positions[seq.id] <- r.i # this is modifying only r_n
end.positions.for.rn <- positions[seq.id] + W - 1
# position is changed only for seq.id row, r_n
mw.data.seq[seq.id, ] <- data.seq[seq.id, positions[seq.id]:end.positions.for.rn]
bg.data.seq[seq.id, ] <- data.seq[seq.id, -c(positions[seq.id]:end.positions.for.rn)]
prob.background <- bg.prob(bg.data.seq)
prob.mw <- 0
i <- 1
while(i != W){ # not a problem since W = 10
prob.mw <- prob.mw +  jth.mw.prob(mw.data.seq, i)
i <- i+1
}
prob.total <- prob.background + prob.mw
prob.vector <- c(prob.vector, prob.total)
}
prob.vector
}
results <- gibbs.sampler(num.iteration = 1)
library(data.table)  # faster computation
?fread
data <- fread("input.csv", sep = " ", stringsAsFactors = T, header = F, nrows = 25)
class(data.table)
class(data)
data <- fread("input.csv", sep = " ", stringsAsFactors = T, header = F, nrows = 25)
K <- 4 # alphabet
W <- 10 # length of a magic word
N <- 25 # number of rows in data == number of sequences
M <- 1000 # length of one sequence
B <- N * (M - W)
alphabet <- c("a", "c", "g", "t")
alpha.bg <- read.table("alphaBg.csv", stringsAsFactors = T, header = F, col.names = "alpha")
alpha.bg <- alpha.bg$alpha # turn into vector
alpha.mw <- read.table("alphaMw.csv", stringsAsFactors = T, header = F, col.names = "alpha")
alpha.mw <- alpha.mw$alpha
data[1: 100:110]
data[1, 100:110]
data[1, 100:110]
data.df <- read.table("input.csv", sep = " ", stringsAsFactors = T, header = F, nrows = 25)
class(data.df)
library(profile)
library(profvis)
profvis({data[1, 150:160]})
profvis({data[1, 150:160]})
data[1, 150:160]
data.df[1, 150:160]
data.df[1, 150:160] %>% table
data.df[1, 1] %>% table
data.df[1, 3] %>% table
data.df[1, 5] %>% table
data.seq <- data.table(data, nrow = N, ncol = M)
View(data.seq)
?data.table
setwd("D:/Materijali/KTH/StatMethods/Projekt/Code/2.1")
library(MCMCpack)
source("/util.R")
library(MCMCpack)
source("util.R")
N <-5
alphabet <-  c("a", "b", "c", "d")
M <- 30
W<- 10
K <- 4
alpha.bg <-  c(1, 1, 1, 1)
alpha.mw<- c(0.8 ,0.8, 0.8, 0.8)
gen.data.list <- generate.data(N, alphabet = c("a", "b", "c", "d"), M, W, alpha.bg,
alpha.mw)
# Extract starting positions
starts <- gen.data.list$starts
B <- N * (M - W)
part1.bg.prob <- lgamma(sum(alpha.bg)) - lgamma(B + sum(alpha.bg))  # checked: it works
part1.mw.prob <- lgamma(sum(alpha.mw)) - lgamma(N*W + sum(alpha.mw))
loggamma.mw <- lgamma(alpha.mw)
loggamma.bg <- lgamma(alpha.bg)
num.chains <- 10
all.chains <- matrix(nrow = 0, ncol = N)
#results.gibbs <- gibbs.sampler(gen.data.list$data, alphabet,  150, N, M, W, length(alphabet), alpha.bg, alpha.mw)
# 1 gibbs.sampler returns 1 chain which was initialized randomly
# but I need to have multiple chains because the initial state is important
for(c in 1:num.chains){
all.chains <- rbind(all.chains, gibbs.sampler(gen.data.list$data, alphabet, 500, N, M, W, length(alphabet), alpha.bg, alpha.mw))
}
library(MCMCpack)
source("util.R")
N <-5
alphabet <-  c("a", "b", "c", "d")
M <- 30
W<- 10
K <- 4
alpha.bg <-  c(1, 1, 1, 1)
alpha.mw<- c(0.8 ,0.8, 0.8, 0.8)
gen.data.list <- generate.data(N, alphabet = c("a", "b", "c", "d"), M, W, alpha.bg,
alpha.mw)
# Extract starting positions
starts <- gen.data.list$starts
B <- N * (M - W)
part1.bg.prob <- lgamma(sum(alpha.bg)) - lgamma(B + sum(alpha.bg))  # checked: it works
part1.mw.prob <- lgamma(sum(alpha.mw)) - lgamma(N*W + sum(alpha.mw))
loggamma.mw <- lgamma(alpha.mw)
loggamma.bg <- lgamma(alpha.bg)
num.chains <- 10
all.chains <- matrix(nrow = 0, ncol = N)
#results.gibbs <- gibbs.sampler(gen.data.list$data, alphabet,  150, N, M, W, length(alphabet), alpha.bg, alpha.mw)
# 1 gibbs.sampler returns 1 chain which was initialized randomly
# but I need to have multiple chains because the initial state is important
for(c in 1:num.chains){
all.chains <- rbind(all.chains, gibbs.sampler(gen.data.list$data, alphabet, 500, N, M, W, length(alphabet), alpha.bg, alpha.mw))
}
library(ggplot2)
library(magrittr)
N <-5
alphabet <-  c("a", "b", "c", "d")
M <- 30
W<- 10
K <- 4
alpha.bg <-  c(1, 1, 1, 1)
alpha.mw<- c(0.8 ,0.8, 0.8, 0.8)
gen.data.list <- generate.data(N, alphabet = c("a", "b", "c", "d"), M, W, alpha.bg,
alpha.mw)
# Extract starting positions
starts <- gen.data.list$starts
B <- N * (M - W)
part1.bg.prob <- lgamma(sum(alpha.bg)) - lgamma(B + sum(alpha.bg))  # checked: it works
part1.mw.prob <- lgamma(sum(alpha.mw)) - lgamma(N*W + sum(alpha.mw))
loggamma.mw <- lgamma(alpha.mw)
loggamma.bg <- lgamma(alpha.bg)
num.chains <- 10
all.chains <- matrix(nrow = 0, ncol = N)
#results.gibbs <- gibbs.sampler(gen.data.list$data, alphabet,  150, N, M, W, length(alphabet), alpha.bg, alpha.mw)
# 1 gibbs.sampler returns 1 chain which was initialized randomly
# but I need to have multiple chains because the initial state is important
for(c in 1:num.chains){
all.chains <- rbind(all.chains, gibbs.sampler(gen.data.list$data, alphabet, 500, N, M, W, length(alphabet), alpha.bg, alpha.mw))
}
setwd("D:/Materijali/KTH/StatMethods/Projekt/Code/2.1")
tibble(counts = as.matrix(all.chains[, 1])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
library(tibble)
tibble(counts = as.matrix(all.chains[, 1])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
tibble(counts = as.matrix(all.chains[, 3])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
tibble(counts = as.matrix(all.chains[, 2])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
tibble(counts = as.matrix(all.chains[, 4])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
tibble(counts = as.matrix(all.chains[((k-1)*(501)+1):(k*501), 4])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
k <- 1
tibble(counts = as.matrix(all.chains[((k-1)*(501)+1):(k*501), 4])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
tibble(counts = as.matrix(all.chains[((k-1)*(501)+1):(k*501), 5])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
tibble(counts = as.matrix(all.chains[((k-1)*(501)+1):(k*501), 4])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
k <- 3
tibble(counts = as.matrix(all.chains[((k-1)*(501)+1):(k*501), 4])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
### Re
k <- 4
tibble(counts = as.matrix(all.chains[((k-1)*(501)+1):(k*501), 4])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
### Re
k <- 6
tibble(counts = as.matrix(all.chains[((k-1)*(501)+1):(k*501), 4])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
### Re
k <- 7
tibble(counts = as.matrix(all.chains[((k-1)*(501)+1):(k*501), 4])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
### Re
k <- 10
tibble(counts = as.matrix(all.chains[((k-1)*(501)+1):(k*501), 4])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
### Rea
k <- 2
tibble(counts = as.matrix(all.chains[((k-1)*(501)+1):(k*501), 4])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
### Re
k <- 3
tibble(counts = as.matrix(all.chains[((k-1)*(501)+1):(k*501), 4])) %>% ggplot(aes(x = counts)) + geom_bar() +
scale_x_continuous(labels = 1:(M - W + 1), breaks = 1:(M - W + 1))
