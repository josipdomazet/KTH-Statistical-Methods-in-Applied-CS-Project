---
title: "StatMethods Project"
author: "Josip Domazet"
date: '03 12 2019 '
header-includes:
   - \usepackage{bm}
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Used libraries
```{r}
library(tidyverse)
library(scales)
library(ggplot2)
library(gridExtra)
library(stratification)
library(MASS)
library(SimDesign)
library(invgamma)
library(ggthemes)
```

In all of the code chunks below, I'm
following the course material as accurately as possible

## 2.2

Function to generate data according to the task


```{r}
# φ = 1:0; σ = 0:16; β = 0:64
# (1, 0.16, 0.64, 100)
generate_data <- function(fi, sigma, beta, total_len){
  require(tidyverse)
  results <- tibble(x = numeric(), y = numeric(), t = numeric())
  x <- rnorm(1, 0, sigma) # x1
  y <- rnorm(1, 0, sqrt(beta^2 * exp(x)))
  results <- tibble::add_row(results, x = x, y = y, t = 1)
  for(i in 2:total_len){
    x <- rnorm(1, fi*x, sigma)
    y <- rnorm(1, 0, sqrt(beta^2 * exp(x)))
    results <- tibble::add_row(results, x = x, y = y, t = i)
  }
  return(results)
}
```


### SIS

The trick is in using the prior of X as
importance distribution:

\[
q(\textbf{x}_{0:t} | \textbf{y}_{1:t}) = p(\textbf{x}_{0:t}) =
p(\textbf{x}_0) \prod_{k=1}^{t} p(\textbf{x}_k |\textbf{x}_{k-1})
\]

```{r}
real_data <- generate_data(1.0, 0.16, 0.64, 100)
real_data_plot <- real_data %>% gather(key = "var", value = "value", c("x", "y"))
ggplot() +
  geom_point(data = real_data_plot %>% filter(var == "y"),
             aes(x = t, y = value, color = var), pch = 8) +
  geom_line(data = real_data_plot %>% filter(var == "x"),
             aes(x = t, y = value, color = var)) +
             scale_color_manual(
             name = "Variable type", 
             breaks = c("x", "y"),
             labels = c("hidden (x)", "observations"),
                                values = c("blue", "red"),
             guide = guide_legend(
               override.aes = list(
                 linetype = c("solid", "blank"),
                 shape = c(NA, 8)
               )
             )) +
  ggtitle("Generated data for T = 100") +
  xlab("T") +
  coord_fixed(ratio = 4)

# https://stackoverflow.com/questions/26587940/ggplot2-different-legend-symbols-for-points-and-lines
```

The problem of filtering: characterising the distribution of the state of
the hidden Markov model at the present time, given the information provided by all of the observations
received up to the present time

Whereas filtering corresponds to estimating the distribution
of the current state of an HMM based upon the observations received up until the current time, 
smoothing corresponds to estimating the distribution of the state at a particular time given all of the 
observations up to some later time

## SIS
```{r}
sis <- function(realy, N){
  
T <- 100 # estimating for t=100

x.pf.sis <- matrix(0, nrow = T, ncol=N) 
w.pf.sis <- matrix(0, nrow = T, ncol=N)
w.norm.sis <- matrix(0, nrow = T, ncol=N)

x.pf.sis[1, ] <- rnorm(N, mean = 0, sd = 0.16) # Initial, t = 1

w.pf.sis[1, ] <- dnorm(realy[1], mean = 0, sd = sqrt(0.64**2 * exp(x.pf.sis[1, ]))) # 
w.norm.sis[1, ] <- w.pf.sis[1, ] / sum(w.pf.sis[1, ])

x.means.sis <- rep(NA, T) 
x.means.sis[1] <- sum(w.norm.sis[1, ] * x.pf.sis[1,])

for(t in 2:T){
  x.pf.sis[t, ] <- rnorm(N, mean = 1.00 * x.pf.sis[t-1, ], sd = 0.16)
  w.pf.sis[t, ] <- dnorm(realy[t], mean = 0, sd = sqrt(0.64^2 * exp(x.pf.sis[t, ]))) * w.pf.sis[t-1, ]
  
  w.norm.sis[t, ] <- w.pf.sis[t, ] / sum(w.pf.sis[t, ])
  x.means.sis[t] <- sum(w.norm.sis[t, ] * x.pf.sis[t, ])
  }
return(list(x.means = x.means.sis, wnorm = w.norm.sis))
}

```

```{r}
results.sis <- sis(real_data$y, N = 100)
```


```{r}
tibble(
  value = as.numeric(results.sis$wnorm[100, ])) %>% 
ggplot(aes(x = value)) +
  geom_histogram(bins = 100, color = "red") +
  ggtitle("Histogram of weights after 100 iterations")
```

```{r}
tibble(t = 1:100, value = results.sis$x.means) %>% 
ggplot(aes(x = t, y = value)) +
  geom_line(aes(color = "a")) +
  geom_line(data = tibble(t = 1:100, x = real_data$x), aes(x = t, y = x, color = "b"),
           alpha = 0.4) +
  ggtitle("Predicted vs real values") +
  scale_color_manual(name = NULL, breaks = c("a", "b"), 
                       labels = c("SIS", "real data"), values = c("red", "blue"))
```


## Bootstrap filter (multinomial resampling)

```{r}
multinomial.resampling <- function(weights.vector){
  # generate N values between 0 and 1
  N <- length(weights.vector)
  u <- rep(NA, N)
  u.tilda <- runif(N)^(1/(1:N))
  u[N] <- u.tilda[N] # set the value for the last element
  k <- N-1
  
  while(k > 0){
    u[k] <- u[k+1] * u.tilda[k]
    k <- k-1
  }
  
  out <- rep(NA, N)
  total <- 0
  i <- 1
  j <- 1
  
  while(j <= N && i <= N){
    total <- total + weights.vector[i]
    while(j <= N && total > u[j]){
      out[j] <- i
      j <- j + 1
    }
    i <- i + 1
  }
  out
}
```



```{r}
BPF.multinomial <- function(realy, sigma, beta, N, T){
particles <- matrix(0, nrow = T, ncol = N) # +1 because of initialisation t=0
particles[1, ] <- rnorm(N, 0, sigma)  # Initial distribution; generate N particles for t = 1

x.means <- matrix(0, nrow=T, ncol=1)
ancestor.indices <- matrix(1, nrow = T, ncol = N) # t = 0, used for weights
log.likelihood <- 0

ancestor.indices[1, ] <- 1:N

weights <- matrix(1, nrow = T, ncol = N)
weights.norm <- matrix(1, nrow= T, ncol=N)

weights[1, ] <- dnorm(realy[1], mean = 0, sd = sqrt(0.64^2 * exp(particles[1, ])), log = T) 
max.weight <- max(weights[1, ]) # max of log values
weights[1, ] <- exp(weights[1, ] - max.weight)
sum_log_weights <- sum(weights[1, ])
weights.norm[1, ] <- weights[1, ] / sum_log_weights

log.likelihood <- log.likelihood + max.weight + log(sum(weights[1, ])) - log(N)
x.means[1, 1] <- sum(weights.norm[1, ] * particles[1, ]) 


for(t in 2:T){
  new.ancestors <- multinomial.resampling(weights.norm[t-1, ])
  #ancestor.indices[1:(t-1), ] <- ancestor.indices[1:(t-1), new.ancestors]
  #ancestor.indices[t, ] <- new.ancestors
  # propagation - generate N new particles for x_{t+1}
  particles[1:(t-1), ] <- particles[1:(t-1), new.ancestors]
  particles[t, ] <- rnorm(N, mean = 1.0*particles[t-1, ], sigma) # using the most important particles 
  
  weights[t, ] <- dnorm(realy[t], 0, sqrt(beta**2 * exp(particles[t, ])), log = TRUE) 

  max.weight <- max(weights[t, ]) # max of log values
  weights[t, ] <- exp(weights[t, ] - max.weight)
  sum_log_weights <- sum(weights[t, ])
  
  weights.norm[t, ] <- weights[t, ] / sum_log_weights
  
  x.means[t, 1]  <- sum(weights.norm[t, ] * particles[t, ])
  
  log.likelihood <- log.likelihood + max.weight +  log(sum(weights[t, ])) - log(N)
}

# Times + 1 is ACTUALLY at t = T
  
  ancestor.index  <- sample(1:N, size=1, prob = weights.norm[T, ])
  x.hat.filtered <- particles[, ancestor.index]

return(list(x.means= x.means, x.hat.filtered = x.hat.filtered, weights.norm = weights.norm, logl = log.likelihood))
}

```


 RUN and compare both methods
```{r}
results.bootstrap <- BPF.multinomial(real_data$y, 0.16,0.64,100,100)
```
 
```{r}


weights.sis.plot <- tibble(
  value = as.numeric(results.sis$wnorm[100, ])) %>% 
ggplot(aes(x = value)) +
  geom_histogram(bins = 50) +
  ggtitle("Histogram of weights after 100 iterations")

weight.boot.multi.plot <- tibble(
  value = as.numeric(results.bootstrap$weights.norm[100, ])) %>% ggplot(aes(x = value)) +
  geom_histogram(bins = 50)

grid.arrange(weights.sis.plot, weight.boot.multi.plot)
```


```{r}

ggplot(data = tibble(t = 1:100, x= as.numeric(results.bootstrap$x.means)), aes(x = t, y = x)) +
  geom_line(aes(color = "green")) +
  geom_line(data = tibble(t = 1:100, x = real_data$x), aes(x = t, y = x, color = "red") , linetype = "dashed") +
  geom_line(data = tibble(t = 1:100, x = results.sis$x.means), aes(t, x, color = "blue")) +
  ggtitle("SIS vs. Bootstrap Filter") +
  scale_color_manual(labels = c("SIS", "Bootstrap \nw/ multinomial\nresampling", "real data"),
                     values = c("#D81B60", "blue", "#004D40"))
```




## Bootstrap with stratified

```{r}
stratified.resampling <- function(weights.vector){
  N <- length(weights.vector)
  output <- rep(NA, N)
  
  total <- weights.vector[1]
  j <- 1
  
  for(i in 1:N){
    u <- (runif(1) + i-1) / N
    while(total < u){
      j <- j + 1
      total <- total + weights.vector[j]
    }
    output[i] <- j
  }
  
  output
}
```


```{r}
BPF.stratified <- function(realy, sigma, beta, N, T){
particles <- matrix(0, nrow = T, ncol = N) # +1 because of initialisation t=0
particles[1, ] <- rnorm(N, 0, sigma)  # Initial distribution; generate N particles for t = 1

x.means <- matrix(0, nrow=T, ncol=1)
ancestor.indices <- matrix(1, nrow = T, ncol = N) # t = 0, used for weights
log.likelihood <- 0

ancestor.indices[1, ] <- 1:N

weights <- matrix(1, nrow = T, ncol = N)
weights.norm <- matrix(1, nrow= T, ncol=N)

weights[1, ] <- dnorm(realy[1], mean = 0, sd = sqrt(0.64^2 * exp(particles[1, ])), log = T) 
max.weight <- max(weights[1, ]) # max of log values
weights[1, ] <- exp(weights[1, ] - max.weight)
sum_log_weights <- sum(weights[1, ])
weights.norm[1, ] <- weights[1, ] / sum_log_weights

log.likelihood <- log.likelihood + max.weight + log(sum(weights[1, ])) - log(N)
x.means[1, 1] <- sum(weights.norm[1, ] * particles[1, ]) 


for(t in 2:T){
  new.ancestors <- stratified.resampling(weights.norm[t-1, ])
  #ancestor.indices[1:(t-1), ] <- ancestor.indices[1:(t-1), new.ancestors]
  #ancestor.indices[t, ] <- new.ancestors
  # propagation - generate N new particles for x_{t+1}
  particles[1:(t-1), ] <- particles[1:(t-1), new.ancestors]
  particles[t, ] <- rnorm(N, mean = 1.0*particles[t-1, ], sigma) # using the most important particles 
  
  weights[t, ] <- dnorm(realy[t], 0, sqrt(beta**2 * exp(particles[t,])), log = TRUE) 

  max.weight <- max(weights[t, ]) # max of log values
  weights[t, ] <- exp(weights[t, ] - max.weight)
  sum_log_weights <- sum(weights[t, ])
  
  weights.norm[t, ] <- weights[t, ] / sum_log_weights
  
  x.means[t, 1]  <- sum(weights.norm[t, ] * particles[t, ])
  
  log.likelihood <- log.likelihood + max.weight +  log(sum(weights[t, ])) - log(N)
}

# Times + 1 is ACTUALLY at t = T
  
  ancestor.index  <- sample(1:N, size=1, prob = weights.norm[T, ])
  x.hat.filtered <- particles[, ancestor.index]

return(list(x.means= x.means, x.hat.filtered = x.hat.filtered, weights.norm = weights.norm, logl = log.likelihood))
}
results.stratified <- BPF.stratified(real_data$y, 0.16,0.64,100,100)
```

## Compare multinomial and stratified resampling 

```{r}
ggplot(data = tibble(t = 1:100, x= as.numeric(results.bootstrap$x.means)), aes(x = t, y = x)) +
  geom_line(aes(color = "green")) +
  geom_line(data = tibble(t = 1:100, x = real_data$x), aes(x = t, y = x, color = "red") , linetype = "dashed") +
  geom_line(data = tibble(t = 1:100, x = as.numeric(results.stratified$x.means)), aes(t, x, color = "blue")) +
  ggtitle("Multinomial vs stratified") +
  scale_color_manual(labels = c("Stratified", "Multinomial\nresampling", "real data"),
                     values = c("#D81B60", "blue", "#004D40"))
```

# Stochastic volatility unknown parameters part I

Still using the same parameters, but we're pretending that  $ \sigma  $ and $ \beta $ are 
both unknown.

Create grid

```{r}
search.grid <- tibble(
  sigma = numeric(),
  beta = numeric(),
  logl = numeric()
)

for(s in seq(0.02,2,by=0.14)){
  for(b in seq(0.02,2,by=0.14)){
    search.grid <- add_row(search.grid, sigma = s, beta = b)
  }
}

```
Log-likelihood
```{r}
loglikelihood <- function(realy, sigma, beta, N, T){
  
  # RUN SMC 10 times
  all.results <- c()
  for(i in 1:10){
  all.results <- c(all.results, BPF.multinomial(realy, sigma, beta, N, T)$logl)
    }
  print(max(all.results))
}

```



```{r}
for(row in 1:nrow(search.grid)){
  search.grid[row,"logl"] <- loglikelihood(realy = real_data$y,
    sigma = search.grid[row,"sigma"] %>% pull, 
    beta = search.grid[row,"beta"] %>% pull,
    100, 100)
}
```


```{r}
search.grid %>% arrange(-logl) %>% top_n(20)
```
```{r}
mlogl <- search.grid %>%  mutate(logl = ifelse(logl > -100, logl, NA)) %>% filter(!is.na(logl)) %>%
summarize(medianlogl = mean(logl)) %>% pull
search.grid %>% mutate(logl = ifelse(logl > -100, logl, NA)) %>% 
ggplot(aes(x = sigma, y = beta, fill = logl)) +
  geom_tile() +scale_fill_gradient2(midpoint = mlogl, low = "blue", mid = "white",
                            high = "red", space = "Lab") +
  xlab(expression(sigma)) + 
  ylab(expression(beta))  +
  theme(axis.title.y = element_text(angle = 0))
```


We can observe that the highest log-likelihood values are 
those most similar to the real values. 
$\sigma = 0.16 $ and $ \beta = 0.72 $ have the highest values.

### Q9
I'll be using $\sigma = 0.16 $ and $ \beta = 0.72 $ and vary 
T and N

First I'll vary T
```{r}
varyT <- tibble(
  Tvalue = numeric(),
  logl = numeric()
)
# Fix N = 100
for(t in c(10, 25, 50,75, 85,100)){
  for(i in 1:30){
  varyT <- add_row(varyT, Tvalue = t, logl = BPF.multinomial(real_data$y, 0.22,0.65, 100, t)$logl)
  }
}

ggplot(varyT, aes(x = Tvalue, group = Tvalue, y = logl+1000)) +
  geom_boxplot() + scale_y_log10()

varyT %>% group_by(Tvalue) %>% summarize(stddev = sd(logl)) %>% print

```


## PHM


```{r}
PMH <- function(y.values, sigma2, beta2, Nparticles, T, Niteration, step.size){

  x.hat <- matrix(0, nrow = T, ncol = Niteration)
  x.hat.proposed <- matrix(0, nrow = T, ncol = Niteration)
  
  theta <- matrix(0, nrow = 2, ncol = Niteration)
  theta.proposed <- matrix(0, nrow = 2, ncol = Niteration)
  
  # log.likelihood is NOT stored, we only keep it for the current time t
  log.likelihood <- matrix(0, nrow = 1, ncol = Niteration)
  log.likelihood.proposed <- matrix(0, nrow = 1, ncol = Niteration)
  # same for acceptance, it is only kept for time = t
  proposed.accepted <- matrix(0, nrow = 1, ncol = Niteration)
  
  # initial values
  theta[1, 1] <- sigma2
  theta[2, 1] <- beta2
  
  results0 <- BPF.multinomial(y.values, sigma=sqrt(theta[1, 1]), beta=sqrt(theta[2, 1]), Nparticles, T)
  log.likelihood[1, 1] <- results0$logl 
  # x.hat[, 1] <- results0$x.hat.filtered
  
  for(k in 2:Niteration){
    
    flag <- 0
    while(flag != 2){
    # theta.proposed[1,k]<- abs(theta[1,k-1] + rnorm(1, 0, step.size))
    # theta.proposed[2,k]<- abs(theta[2,k-1] + rnorm(1, 0, step.size))
      theta.proposed[, k] <- rmvnorm(1, mean = theta[, k-1], sigma = step.size)
      flag <- sum(theta.proposed[, k] > 0.00)
    }

   # if(theta.proposed[1, k] > 0.00 && theta.proposed[2, k] > 0.00){ 
      results <- BPF.multinomial(y.values, sigma=sqrt(theta.proposed[1, k]), beta=sqrt(theta.proposed[2, k]), Nparticles, T)
      log.likelihood.proposed[1, k] <- results$logl
     # x.hat.proposed[, k] <- results$x.hat.filtered
    #} 
    
    # mistake! it's sigma^2, not sigma
  
    prior.sigma <- invgamma::dinvgamma(theta.proposed[1, k], shape = 0.01, rate = 0.01, log = T)
    diff.sigma <- prior.sigma - invgamma::dinvgamma(theta[1, k-1], shape = 0.01, rate = 0.01, log = T)

    prior.beta <- invgamma::dinvgamma(theta.proposed[2, k], shape = 0.01, rate = 0.01, log = T)
    diff.beta <- prior.beta - invgamma::dinvgamma(theta[2, k-1], shape = 0.01, rate = 0.01, log = T)

    prior.diff.sum <- diff.sigma + diff.beta
    # acceptance probability

    likelihood.dif <- log.likelihood.proposed[1, k] - log.likelihood[1, k-1]
    accept.probability <- exp(prior.diff.sum + likelihood.dif)

    #accept.probability <- accept.probability * (theta.proposed[1, k] > 0.00) # sd is positive!
    #accept.probability <- accept.probability * (theta.proposed[2, k] > 0.00) # beta also

    uniform <- runif(1)
    if(uniform < accept.probability){ # accept new parameters
      theta[1:2, k] <- theta.proposed[1:2, k] 
      log.likelihood[1, k] <- log.likelihood.proposed[1, k]
    #  x.hat[, k] <- x.hat.proposed[, k]
      proposed.accepted[1, k] <- 1
    } else {
      theta[1:2, k] <- theta[1:2, k-1] # note! it is not PROPOSED theta
      log.likelihood[1, k] <- log.likelihood[1, k-1]
     # x.hat[, k] <- x.hat[, k-1]
      proposed.accepted[1, k] <- 0
    }
  }
  # theta is matrix 
  return(list(theta = theta, proposed.accepted = proposed.accepted))
  
}
```

```{r}
Niteration <- 1500
PMH.results <- PMH(real_data$y, sigma2 = .5, beta2 = .1, Nparticles = 100, step.size = diag(c(0.5, 0.5)), T = 100, Niteration = Niteration)
PMH.posterior.sigma2 <- tibble(sigma2 = PMH.results$theta[1,1:Niteration])
PMH.posterior.beta2 <- tibble(beta2 = PMH.results$theta[2,1:Niteration])
```



```{r}
ggplot(PMH.posterior.sigma2, aes(x = sigma2)) + 
  geom_histogram(bins = 100, alpha = 0.9, aes(fill = ..count..))  +
 # geom_vline(xintercept = round(mean(PMH.posterior.sigma2$sigma2), 2), size = 1.2, linetype = 3, color = "#d15973") +
  xlab(expression(sigma^{2})) +
  ggtitle( expression(paste("Distribution of ", sigma^{2}))) +
  annotate('text', x = 0.28, y = 500, 
        label = paste("bar(sigma^2)==~",round(mean(PMH.posterior.sigma2$sigma2), 4)),
        parse = TRUE,size=10) 
```

```{r}
ggplot(PMH.posterior.beta2, aes(x = beta2)) + 
  geom_histogram(bins=100, aes(fill = ..count..)) +
  #geom_vline(xintercept = 0.41, size = 1.5, linetype = 4, color = "yellow") +
  xlab(expression(beta^{2})) +
  ggtitle(expression(paste("Distribution of ", beta^{2}))) +
  annotate('text', x = 0.25, y = 250, 
        label = paste("bar(beta^2)==~",round(mean(PMH.posterior.beta2$beta2), 4)),
        parse = TRUE,size=8) 
```

Maximum a posteriori
```{r}
beta2values <- hist(PMH.posterior.beta2$beta2, breaks=seq(0, 2, by=0.1), plot=FALSE)
beta2values$breaks[which(beta2values$counts == max(beta2values$counts))]
```



```{r}
tibble(sigma2 = PMH.results$theta[1,]) %>% mutate(iterations = 1:Niteration) %>% ggplot(aes(x = iterations,
                                                                y = sigma2)) +
  geom_line()
```



```{r}
tibble(beta2 = PMH.results$theta[2,]) %>% mutate(iterations = 1:Niteration) %>% ggplot(aes(x = iterations,
                                                                y = beta2)) +
  geom_line()
```

## Gibbs Sampling

### Conditional SMC

```{r}
csmc <- function(x.values, theta, y.values, num.particles, T){
  sigma <- theta[1]
  beta <- theta[2]
  
  particles <- matrix(0, nrow = T, ncol = num.particles)
  weights <- matrix(0, nrow = T, ncol = num.particles)
  weights.norm <- matrix(0, nrow = T , ncol = num.particles)
  
  particles[1, ] <- rnorm(num.particles, 0, sigma) 
  particles[1, num.particles] <- x.values[1] 
  
  ancestor.indices <- matrix(0, nrow = T, ncol = num.particles)
  ancestor.indices[1, ] <- 1:num.particles
  
  
  weights[1, ] <- dnorm(y.values[1], mean = 0, sd = sqrt(beta^2 * exp(particles[1, ])))
  weights.norm[1, ] <- weights[1, ] / sum(weights[1, ])
  
  x.means <- matrix(0, nrow = T, ncol = 1)
  
  x.means[1, ] <- sum(weights.norm[1, ] * particles[1, ])
  
  for(t in 2:T){
    
    # new.ancestors <- stratified.resampling(
    #   weights.norm[t-1, 1:(num.particles-1)] /   sum(weights.norm[t-1,1:(num.particles-1)]))
    new.ancestors <- multinomial.resampling(
      weights.norm[t-1, ])
    
    ancestor.indices[t, ] <- c(ancestor.indices[t-1, new.ancestors])
    ancestor.indices[t, num.particles] <- num.particles
    
    particles[t, ] <- rnorm(num.particles, mean = 1.0*particles[t-1, new.ancestors], sd=sigma)
    particles[t, num.particles] <-  x.values[t] # replace last one
    
    weights[t, ] <- dnorm(y.values[t], mean = 0, sd = sqrt(beta^2 * exp(particles[t, ])))
    weights.norm[t, ] <- weights[t, ] / sum(weights[t, ])
    
    x.means[t, ] <- sum(weights.norm[t, ] * particles[t, ])
  } 
  
  ancestor.index  <- sample(1:num.particles, size=1, prob = weights.norm[T, ]) 
  x.hat.filtered <- particles[cbind(1:T, ancestor.indices[, ancestor.index])]
  
  list(x.hat.filtered = x.hat.filtered, x.means = x.means)
  
}


```

What should be the values for x*?
```{r}
results.csmc <- csmc(rnorm(100), theta=c(0.16, 0.64), real_data$y, 100, 100)
plot(results.csmc$x.hat.filtered)
plot(results.csmc$x.means)
plot(real_data$x)
```



```{r}
particle.gibbs <- function(y.values, theta, T, num.particles, num.iteration){
  current.sigma2 <- theta[1]
  current.beta2 <- theta[2]
  
  # set x values arbitrarily
  current.state <- generate_data(1, sqrt(current.sigma2),sqrt(current.beta2),100) %>% pull(x)

    store.sigma2 <- tibble(sigma2 = numeric())
  store.beta2 <- tibble(beta2 = numeric())
  
  store.sigma2  <- add_row(store.sigma2, sigma2 = current.sigma2)
  store.beta2<- add_row(store.beta2, beta2 = current.beta2)
  
  for(k in 2:num.iteration){
    
    current.state <- c(0, current.state)
    
    current.sigma2 <- rinvgamma(n = 1, shape = 0.01 + T/2,
                                rate = 0.01 + 0.5 * sum((current.state[2:(T+1)] - current.state[1:T])^2)) 
    
    current.beta2 <- rinvgamma(n = 1, shape = 0.01 + T/2, 
                               rate = 0.01 + 0.5 * sum(exp(-current.state[2:(T+1)])*((y.values[1:T])^2)))
    
    store.sigma2 <- add_row(store.sigma2, sigma2 = sqrt(current.sigma2))
    store.beta2 <- add_row(store.beta2, beta2 = sqrt(current.beta2))
    # use these new parametrs in CSMC
    
    current.state <- as.numeric(csmc(current.state, theta=c(sqrt(current.sigma2), sqrt(current.beta2)), y.values, 100, 100)$x.hat.filtered) # this is the main problem: using x.hat.filtered causes too large values
    
  }
  
  list(a = store.sigma2, b = store.beta2)
  
}


```

```{r}
results.gibbs <- particle.gibbs(real_data$y, c(.2, .2), 100, 100, 0)
```


```{r}
ggplot(results.gibbs$a, aes(sigma2)) +
  geom_histogram()
```


```{r}
ggplot(results.gibbs$b, aes(beta2)) +
  geom_histogram() 
```






